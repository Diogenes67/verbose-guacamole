import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, roc_curve
)
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency

# Load the dataset (Google stock prices here)
file_path = '/Users/anthonyporter/Desktop/Deep Learning/archive(5)/Google_Stock_Price_Train.csv'
df = pd.read_csv(file_path)

# Clean and preprocess the data: make 'Close' and 'Volume' usable
df['Close'] = pd.to_numeric(df['Close'].str.replace(r'[^\d.]', '', regex=True))
df['Volume'] = pd.to_numeric(df['Volume'].str.replace(r'[^\d]', '', regex=True))

# Add moving averages to smooth out the price trends
df['MA_5'] = df['Close'].rolling(window=5).mean().fillna(df['Close'])
df['MA_10'] = df['Close'].rolling(window=10).mean().fillna(df['Close'])

# Prepare features and labels for model input/output
features = df[['Open', 'High', 'Low', 'Close', 'Volume', 'MA_5', 'MA_10']].values
closing_prices = df['Close'].values

# Labels are based on whether the price jumps >5% in the next 48 hours
future_window = 48
threshold = 0.05
labels = [
    1 if (closing_prices[i + future_window] - closing_prices[i]) / closing_prices[i] > threshold else 0
    for i in range(len(closing_prices) - future_window)
]

# Match features and labels – they need to align length-wise
features = features[:-future_window]
labels = np.array(labels)

# Scale the features to 0-1 range (important for GRU training stability)
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(features)

# Turn data into sequences for GRU input
SEQ_LENGTH = 30
def create_sequences(data, targets, seq_length):
    sequences = [data[i:i + seq_length] for i in range(len(data) - seq_length)]
    sequence_labels = [targets[i + seq_length] for i in range(len(data) - seq_length)]
    return np.array(sequences), np.array(sequence_labels)

X, y = create_sequences(scaled_features, labels, SEQ_LENGTH)

# Train-test split (standard ML step)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors (GRU needs these)
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# PyTorch Dataset class – helps with batching
class StockDataset(Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx], self.targets[idx]

train_loader = DataLoader(StockDataset(X_train_tensor, y_train_tensor), batch_size=16, shuffle=True)
test_loader = DataLoader(StockDataset(X_test_tensor, y_test_tensor), batch_size=16, shuffle=False)

# GRU model with dropout to prevent overfitting
class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=32, num_layers=2, dropout=0.3):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        _, h_n = self.gru(x)  # Only need the last hidden state
        return torch.sigmoid(self.fc(h_n[-1]))

model = GRUModel(input_dim=X_train.shape[2])

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Training loop with early stopping to prevent overfitting
EPOCHS = 100
PATIENCE = 10
best_val_loss = float('inf')
wait = 0

train_losses, val_losses = [], []

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch).squeeze()
        loss = criterion(outputs, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        train_loss += loss.item()
    train_losses.append(train_loss / len(train_loader))
    
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch).squeeze()
            val_loss += criterion(outputs, y_batch).item()
    val_losses.append(val_loss / len(test_loader))
    
    print(f"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}")
    
    # Check if validation loss improves
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        wait = 0
    else:
        wait += 1
        if wait >= PATIENCE:
            print("Stopping early due to no improvement.")
            break

# Plot loss curves to monitor performance
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.legend()
plt.title("Loss Curves")
plt.show()

# Evaluate the model on the test set
model.eval()
y_probs = []
y_true = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        preds = model(X_batch).squeeze()
        y_probs.extend(preds.tolist())
        y_true.extend(y_batch.tolist())

# Use ROC curve to find the optimal threshold
fpr, tpr, thresholds = roc_curve(y_true, y_probs)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
y_pred = (np.array(y_probs) > optimal_threshold).astype(int)

# Compute evaluation metrics
metrics = {
    'accuracy': accuracy_score(y_true, y_pred),
    'precision': precision_score(y_true, y_pred),
    'recall': recall_score(y_true, y_pred),
    'f1': f1_score(y_true, y_pred),
    'roc_auc': roc_auc_score(y_true, y_probs),
}
print(f"Evaluation Metrics: {metrics}")

# Plot the confusion matrix for a clearer view of predictions
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')
plt.title("Confusion Matrix")
plt.show()

# Chi-squared test to analyze confusion matrix results
chi2, p, dof, expected = chi2_contingency(cm)
print(f"Chi-squared Test: χ² = {chi2:.4f}, p = {p:.4f}")
