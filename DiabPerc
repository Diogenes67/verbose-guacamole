import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the dataset
# dataset with health metrics, including a binary outcome var
file_path = '/Users/anthonyporter/Desktop/Deep Learning/1/diabetes.csv'
diabetes_data = pd.read_csv(file_path)

# Step 2: Define Perceptron model class (no hidden layers)
# simple model to get an understanding of how binary classification works

class BasicPerceptron(nn.Module):
    def __init__(self, input_size=8):
        super(BasicPerceptron, self).__init__()
        # Single-layer neural network (no hidden layers)
        self.fc1 = nn.Linear(input_size, 1)  # Linear transformation (input -> output)
    
    def forward(self, x):
        # ended up changing to sigmoid later
        return self.fc1(x)  # Raw output (logits)

# Step 3: Train the model function
#  iterate over multiple epochs, adjust the weights, and minimize loss
def train_model(model, optimizer, criterion, X_train, y_train, epochs=100):
    # For each epoch, we perform forward and backward passes, and adjust the model's weights
    for epoch in range(epochs):
        model.train()  # Set the model to training mode
        y_pred = model(X_train)  # Forward pass to get predictions
        loss = criterion(y_pred, y_train)  # Calculate loss (binary cross-entropy)

        # Backpropagation and weight update
        optimizer.zero_grad()  # Zero the gradients
        loss.backward()  # Backpropagate the loss
        optimizer.step()  # Adjust weights

        # Print loss at regular intervals 
        if epoch % 10 == 0:
            print(f'Epoch {epoch}/{epochs} - Loss: {loss.item():.4f}')

# Step 4: KNN Imputation
# Handling missing data: Missing values are imputed using KNN to retain more information than mean/median imputation
def knn_imputation(data, n_neighbors):
    imputer = KNNImputer(n_neighbors=n_neighbors)
    return pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

# Step 5: Final run - Train the model and evaluate
def final_run_and_train(n_neighbors, learning_rate, epochs):
    # Reload the dataset (to start fresh)
    data = pd.read_csv(file_path)
    
    # Handling zeros in specific columns as missing data 
    columns_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    data[columns_to_impute] = data[columns_to_impute].replace(0, np.nan)

    # Apply KNN Imputation to handle missing values
    data_imputed = knn_imputation(data, n_neighbors)

    # Normalize the data to improve training stability (gradient descent works better when features are scaled)
    X = data_imputed.drop(columns=['Outcome'])
    y = data_imputed['Outcome']
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

    # Stratified 80/20 train-test split to preserve class balance in both training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, stratify=y)
    
    # Convert data to tensors for PyTorch
    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

    # Initialize the model and optimizer
    model = BasicPerceptron()  # Using a simple Perceptron without hidden layers
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # Stochastic Gradient Descent
    criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss with logits 

    # Train the model
    train_model(model, optimizer, criterion, X_train_tensor, y_train_tensor, epochs=epochs)
    
    return model, X_test_tensor, y_test_tensor

# Step 6: Plot Confusion Matrix
# Visualizing the confusion matrix helps understand the classification performance (false positives/negatives)
def plot_confusion_matrix(model, X_test_tensor, y_test_tensor):
    y_pred_tensor = torch.sigmoid(model(X_test_tensor)) >= 0.5  # Sigmoid to convert logits to probabilities
    y_pred = y_pred_tensor.cpu().numpy()
    y_true = y_test_tensor.cpu().numpy()

    # Compute the confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Plot the confusion matrix for easier interpretation
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

# Step 7: Precision, Recall, and F1-Score Analysis
# Classification metrics (precision, recall, F1-score) provide a more detailed view of performance 
def classification_report_analysis(model, X_test_tensor, y_test_tensor):
    y_pred_tensor = torch.sigmoid(model(X_test_tensor)) >= 0.5  # Apply threshold
    y_pred = y_pred_tensor.cpu().numpy()
    y_true = y_test_tensor.cpu().numpy()

    # Print classification report (precision, recall, F1-score)
    report = classification_report(y_true, y_pred, target_names=['Non-Diabetic', 'Diabetic'])
    print(report)

# Step 8: ROC Curve and AUC Analysis
# ROC curve helps in evaluating the trade-off between sensitivity and specificity
def plot_roc_curve_and_auc(model, X_test_tensor, y_test_tensor):
    y_pred_prob = torch.sigmoid(model(X_test_tensor)).detach().cpu().numpy()  # Get predicted probabilities
    y_true = y_test_tensor.cpu().numpy()

    # Compute ROC curve and AUC score
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)
    auc = roc_auc_score(y_true, y_pred_prob)

    # Plot the ROC curve
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc:.4f})")
    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random chance)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend(loc="lower right")
    plt.show()

# Step 9: Analyze Model Stability
# Running the model multiple times to understand how sensitive the model is to data splits
def analyze_stability(accuracies):
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    
    print(f"Mean Accuracy: {mean_accuracy:.4f}")
    print(f"Standard Deviation: {std_dev:.4f}")

# Step 10: Final Model Evaluation
# Using previously identified hyperparameters using grid search (not shown in this version)
best_learning_rate = 0.01
best_epochs = 400
best_n_neighbors = 7

# Run final model
model, X_test_tensor, y_test_tensor = final_run_and_train(best_n_neighbors, best_learning_rate, best_epochs)

# Plot confusion matrix to evaluate performance
plot_confusion_matrix(model, X_test_tensor, y_test_tensor)

# Show detailed classification metrics (precision, recall, F1-score)
classification_report_analysis(model, X_test_tensor, y_test_tensor)

# Plot ROC curve and calculate AUC score for overall performance
plot_roc_curve_and_auc(model, X_test_tensor, y_test_tensor)

   
