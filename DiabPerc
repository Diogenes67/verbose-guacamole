import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the dataset
file_path = '/Users/anthonyporter/Desktop/Deep Learning/1/diabetes.csv'
diabetes_data = pd.read_csv(file_path)

# Step 2: Define the model class (no hidden layers)
class BasicPerceptron(nn.Module):
    def __init__(self, input_size=8):
        super(BasicPerceptron, self).__init__()
        self.fc1 = nn.Linear(input_size, 1)  # Single-layer perceptron
    
    def forward(self, x):
        return self.fc1(x)  # Raw output, no activation function here

# Step 3: Train the model
def train_model(model, optimizer, criterion, X_train, y_train, epochs=100):
    for epoch in range(epochs):
        model.train()
        y_pred = model(X_train)
        loss = criterion(y_pred, y_train)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Step 4: KNN Imputation
def knn_imputation(data, n_neighbors):
    imputer = KNNImputer(n_neighbors=n_neighbors)
    return pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

# Step 5: Final Run - Train the model and evaluate
def final_run_and_train(n_neighbors, learning_rate, epochs):
    # Reload the dataset
    data = pd.read_csv(file_path)
    columns_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    data[columns_to_impute] = data[columns_to_impute].replace(0, np.nan)

    # Apply KNN Imputation
    data_imputed = knn_imputation(data, n_neighbors)

    # Normalize the data
    X = data_imputed.drop(columns=['Outcome'])
    y = data_imputed['Outcome']
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

    # Stratified train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, stratify=y)
    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

    # Initialize model and optimizer
    model = BasicPerceptron()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    criterion = nn.BCEWithLogitsLoss()

    # Train the model
    train_model(model, optimizer, criterion, X_train_tensor, y_train_tensor, epochs=epochs)
    
    return model, X_test_tensor, y_test_tensor

# Step 6: Confusion Matrix
def plot_confusion_matrix(model, X_test_tensor, y_test_tensor):
    y_pred_tensor = torch.sigmoid(model(X_test_tensor)) >= 0.5  # Predict
    y_pred = y_pred_tensor.cpu().numpy()
    y_true = y_test_tensor.cpu().numpy()

    # Compute the confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Plot the confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

# Step 7: Precision, Recall, F1-Score
def classification_report_analysis(model, X_test_tensor, y_test_tensor):
    y_pred_tensor = torch.sigmoid(model(X_test_tensor)) >= 0.5  # Predict
    y_pred = y_pred_tensor.cpu().numpy()
    y_true = y_test_tensor.cpu().numpy()

    # Print classification report
    report = classification_report(y_true, y_pred, target_names=['Non-Diabetic', 'Diabetic'])
    print(report)

# Step 8: ROC Curve and AUC
def plot_roc_curve_and_auc(model, X_test_tensor, y_test_tensor):
    # Detach the tensor from the computational graph and convert it to a NumPy array
    y_pred_prob = torch.sigmoid(model(X_test_tensor)).detach().cpu().numpy()  # Get probabilities
    y_true = y_test_tensor.cpu().numpy()

    # Compute ROC curve and AUC
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)
    auc = roc_auc_score(y_true, y_pred_prob)

    # Plot ROC curve
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc:.4f})")
    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random chance)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend(loc="lower right")
    plt.show()


# Step 9: Model Stability Analysis (Standard Deviation of Accuracy)
def analyze_stability(accuracies):
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    
    print(f"Mean Accuracy: {mean_accuracy:.4f}")
    print(f"Standard Deviation: {std_dev:.4f}")

# Step 10: Final Evaluation
# Use the best hyperparameters from your grid search
best_learning_rate = 0.01
best_epochs = 400
best_n_neighbors = 7

# Run the final model
model, X_test_tensor, y_test_tensor = final_run_and_train(best_n_neighbors, best_learning_rate, best_epochs)

# Plot the confusion matrix
plot_confusion_matrix(model, X_test_tensor, y_test_tensor)

# Show precision, recall, F1-score
classification_report_analysis(model, X_test_tensor, y_test_tensor)

# Plot ROC curve and AUC
plot_roc_curve_and_auc(model, X_test_tensor, y_test_tensor)
